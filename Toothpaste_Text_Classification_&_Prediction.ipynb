{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nMzI-3E2KQdfTFKqV29fDaYwHBJMfe59",
      "authorship_tag": "ABX9TyPk2pguNryKsD025JN7kzC2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhazulamin1/Toothpaste-Customer-Review-Text-Classification-Prediction/blob/main/Toothpaste_Text_Classification_%26_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwjbZQG4FFh9",
        "outputId": "6d6ec587-65ef-447f-baec-3633a80cca25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199773 sha256=d0b55afd5a06b2353173966c462889dcca4888c3af72d5313ccbd77631f44d5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25wYCc_txwHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd648827-a5ce-408d-c7cb-d3d3cada2b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing top 5 rows of dataframe showing original and cleaned texts....\n",
            "                                                                                                                                                                                                                 Text  \\\n",
            "0                                  pleasant aroma gentle on gums quality product improves oral health perfect for daily use refreshing breath long-lasting effect improves oral health whitens teeth pleasant aroma ðŸ‘Ž   \n",
            "1                                 quality product pleasant aroma pleasant taste perfect for daily use improves oral health refreshing breath eco-friendly packaging gentle on gums refreshing breath gentle on gums ðŸ‘Ž   \n",
            "2                              long-lasting effect good value pleasant aroma perfect for daily use gentle on gums whitens teeth eco-friendly packaging quality product pleasant taste pleasant aroma pleasant aroma ðŸŒ¿   \n",
            "3                             highly recommend refreshing breath perfect for daily use gentle on gums long-lasting effect pleasant aroma highly recommend pleasant aroma highly recommend good value pleasant aroma ðŸ˜Š   \n",
            "4  pleasant taste pleasant aroma whitens teeth refreshing breath gentle on gums gentle on gums highly recommend eco-friendly packaging highly recommend refreshing breath quality product quality #NaturalIngredients   \n",
            "\n",
            "                                                                                                                                                                                                               cleaned_review  \n",
            "0                                    [pleasant, aroma, gentle, gum, quality, product, improve, oral, health, perfect, daily, use, refresh, breath, long, last, effect, improve, oral, health, whiten, teeth, pleasant, aroma]  \n",
            "1                                        [quality, product, pleasant, aroma, pleasant, taste, perfect, daily, use, improve, oral, health, refresh, breath, eco, friendly, package, gentle, gum, refresh, breath, gentle, gum]  \n",
            "2                            [long, last, effect, good, value, pleasant, aroma, perfect, daily, use, gentle, gum, whiten, teeth, eco, friendly, package, quality, product, pleasant, taste, pleasant, aroma, pleasant, aroma]  \n",
            "3                            [highly, recommend, refresh, breath, perfect, daily, use, gentle, gum, long, last, effect, pleasant, aroma, highly, recommend, pleasant, aroma, highly, recommend, good, value, pleasant, aroma]  \n",
            "4  [pleasant, taste, pleasant, aroma, whiten, teeth, refresh, breath, gentle, gum, gentle, gum, highly, recommend, eco, friendly, package, highly, recommend, refresh, breath, quality, product, quality, naturalingredients]  \n",
            "Shape of tfidf matrix:  (1000, 593)\n",
            "Iteration  1\n",
            "Iteration 1, Recall: 0.8979591836734694, Precision: 0.88\n",
            "Iteration  2\n",
            "Iteration 2, Recall: 0.9387755102040817, Precision: 0.8214285714285714\n",
            "Iteration  3\n",
            "Iteration 3, Recall: 0.92, Precision: 0.9019607843137255\n",
            "Iteration  4\n",
            "Iteration 4, Recall: 0.98, Precision: 0.9423076923076923\n",
            "Iteration  5\n",
            "Iteration 5, Recall: 0.88, Precision: 0.8627450980392157\n",
            "Iteration  6\n",
            "Iteration 6, Recall: 0.92, Precision: 0.9019607843137255\n",
            "Iteration  7\n",
            "Iteration 7, Recall: 0.9, Precision: 0.8823529411764706\n",
            "Iteration  8\n",
            "Iteration 8, Recall: 0.9, Precision: 0.8333333333333334\n",
            "Iteration  9\n",
            "Iteration 9, Recall: 0.94, Precision: 0.8703703703703703\n",
            "Iteration  10\n",
            "Iteration 10, Recall: 0.8, Precision: 0.9302325581395349\n",
            "Mean cross-validation recall (Nearest Centroid):  0.9076734693877553\n",
            "Mean cross-validation precision (Nearest Centroid):  0.8826692133422641\n",
            "Iteration  1\n",
            "Iteration 1, Recall: 0.89, Precision: 0.88\n",
            "Iteration  2\n",
            "Iteration 2, Recall: 0.87, Precision: 0.8214285714285714\n",
            "Iteration  3\n",
            "Iteration 3, Recall: 0.89, Precision: 0.8979591836734694\n",
            "Iteration  4\n",
            "Iteration 4, Recall: 0.96, Precision: 0.9423076923076923\n",
            "Iteration  5\n",
            "Iteration 5, Recall: 0.88, Precision: 0.88\n",
            "Iteration  6\n",
            "Iteration 6, Recall: 0.89, Precision: 0.8979591836734694\n",
            "Iteration  7\n",
            "Iteration 7, Recall: 0.89, Precision: 0.8823529411764706\n",
            "Iteration  8\n",
            "Iteration 8, Recall: 0.86, Precision: 0.8461538461538461\n",
            "Iteration  9\n",
            "Iteration 9, Recall: 0.89, Precision: 0.8679245283018868\n",
            "Iteration  10\n",
            "Iteration 10, Recall: 0.88, Precision: 0.9318181818181818\n",
            "Mean cross-validation recall (FastText):  0.89\n",
            "Mean cross-validation precision (FastText):  0.8847904128533587\n"
          ]
        }
      ],
      "source": [
        "import re, nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "import fasttext\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "# Reading dataset as dataframe\n",
        "df = pd.read_csv(\"/content/Labelled.csv\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Converting structured categorical features to numerical features\n",
        "df['Label'] = df['Label'].map({'Positive':0, 'Negative':1})\n",
        "\n",
        "# Cleaning Review\n",
        "def cleaner(Text):\n",
        "    if pd.isna(Text):\n",
        "        return []\n",
        "    soup = BeautifulSoup(Text, 'lxml')  # removing HTML entities such as â€˜&ampâ€™,â€™&quotâ€™,'&gt'; lxml is the html parser and should be installed using 'pip install lxml'\n",
        "    souped = soup.get_text()\n",
        "    re1 = re.sub(r\"(@|http://|https://|Ã°|Å¸|Å’|Ëœ|Å |Å¾|www|\\\\x)\\S*\", \" \", souped)  # substituting @mentions, urls, etc with whitespace\n",
        "    re2 = re.sub(\"[^A-Za-z]+\", \" \", re1)  # substituting any non-alphabetic character that repeats one or more times with whitespace\n",
        "\n",
        "    tokens = nltk.word_tokenize(re2)\n",
        "    lower_case = [t.lower() for t in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t, 'v') for t in filtered_result]\n",
        "    return lemmas\n",
        "\n",
        "df['cleaned_review'] = df.Text.apply(cleaner)\n",
        "df = df[df['cleaned_review'].map(len) > 0] #removing rows with cleaned texts of length 0\n",
        "print(\"Printing top 5 rows of dataframe showing original and cleaned texts....\")\n",
        "print(df[['Text','cleaned_review']].head())\n",
        "df.drop(['Text'], axis=1, inplace=True)\n",
        "# Saving cleaned text to csv\n",
        "df.to_csv('cleaned_data.csv', index=False)\n",
        "df['cleaned_review'] = [\" \".join(row) for row in df['cleaned_review'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n",
        "data = df['cleaned_review']\n",
        "Y = df['Label'] # target column\n",
        "tfidf = TfidfVectorizer(min_df=.03, ngram_range=(1,3)) # min_df=.03 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (0.03 * 1000 = 30).\n",
        "tfidf.fit(data) # learn vocabulary of entire data\n",
        "data_tfidf = tfidf.transform(data) # creating tfidf values\n",
        "pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary.csv', header=False, index=False)\n",
        "print(\"Shape of tfidf matrix: \", data_tfidf.shape)\n",
        "\n",
        "# Implementing Nearest Centroid classifier\n",
        "model = NearestCentroid('cosine')  # cosine - Assesing Angle-based similarity, length invariance\n",
        "\n",
        "# Suppress the specific warning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.neighbors._nearest_centroid\")\n",
        "\n",
        "# Running cross-validation\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n",
        "nc_scores = []\n",
        "iteration = 0\n",
        "for train_index, test_index in kf.split(data_tfidf, Y):\n",
        "    iteration += 1\n",
        "    print(\"Iteration \", iteration)\n",
        "    X_train, Y_train = data_tfidf[train_index], Y[train_index]\n",
        "    X_test, Y_test = data_tfidf[test_index], Y[test_index]\n",
        "    model.fit(X_train, Y_train) # Fitting the Nearest Centroid Classifier\n",
        "\n",
        "    Y_pred = model.predict(X_test)\n",
        "    recall = recall_score(Y_test, Y_pred)\n",
        "    precision = precision_score(Y_test, Y_pred)\n",
        "    nc_scores.append((recall, precision))\n",
        "    print(f\"Iteration {len(nc_scores)}, Recall: {recall}, Precision: {precision}\")\n",
        "\n",
        "mean_recall, mean_precision = np.mean(nc_scores, axis=0)\n",
        "print(\"Mean cross-validation recall (Nearest Centroid): \", mean_recall)\n",
        "print(\"Mean cross-validation precision (Nearest Centroid): \", mean_precision)\n",
        "\n",
        "\n",
        "# Implementing FastText\n",
        "# Function to write data to file in FastText format\n",
        "def write_fasttext_file(filename, X, Y):\n",
        "    with open(filename, 'w') as file:\n",
        "        for text, label in zip(X, Y):\n",
        "            line = f'__label__{label} {text}\\n'\n",
        "            file.write(line)\n",
        "\n",
        "# Implementing Stratified K-Fold for cross-validation with FastText\n",
        "ft_scores = []\n",
        "iteration = 0\n",
        "\n",
        "for train_index, test_index in kf.split(data, Y):\n",
        "    iteration += 1\n",
        "    print(\"Iteration \", iteration)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
        "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
        "\n",
        "    # Write training and testing data to files\n",
        "    train_file = 'fasttext_train.txt'\n",
        "    test_file = 'fasttext_test.txt'\n",
        "    write_fasttext_file(train_file, X_train, Y_train)\n",
        "    write_fasttext_file(test_file, X_test, Y_test)\n",
        "\n",
        "    # Train FastText model\n",
        "    model = fasttext.train_supervised(input=train_file,\n",
        "                                      lr=0.5,           # Learning rate to balances fast learning without overshooting\n",
        "                                      epoch=25,         # Number of epochs to create sufficient iterations for convergence\n",
        "                                      dim=100,          # Vector dimension for adequate detail, manageable complexity\n",
        "                                      minCount=2)       # Minimum count to filters out rare words\n",
        "\n",
        "   # Evaluate model\n",
        "    result = model.test(test_file)\n",
        "    recall = result[1]\n",
        "    Y_pred = [model.predict(text)[0][0] == '__label__1' for text in X_test]\n",
        "    precision = precision_score(Y_test, Y_pred)\n",
        "    ft_scores.append((recall, precision))\n",
        "    print(f\"Iteration {len(ft_scores)}, Recall: {recall}, Precision: {precision}\")\n",
        "\n",
        "mean_recall, mean_precision = np.mean(ft_scores, axis=0)\n",
        "print(\"Mean cross-validation recall (FastText): \", mean_recall)\n",
        "print(\"Mean cross-validation precision (FastText): \", mean_precision)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# data_tfidf,Y = smote.fit_resample(data_tfidf,Y)\n",
        "# clf = fasttext().fit(data_tfidf, Y)\n",
        "# joblib.dump(clf, 'svc.sav')\n",
        "\n",
        "model.save_model('fasttext_model.bin')"
      ],
      "metadata": {
        "id": "aAjJTgzgK2U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "\n",
        "# model = joblib.load('/content/svc.sav')\n",
        "model.save_model('fasttext_model.bin')\n",
        "vocabulary = pd.read_csv('/content/vocabulary.csv', header=None)\n",
        "vocabulary_dict = {}\n",
        "for i, word in enumerate(vocabulary[0]):\n",
        "      vocabulary_dict[word] = i\n",
        "print(vocabulary_dict)\n",
        "tfidf = TfidfVectorizer(vocabulary = vocabulary_dict,lowercase=False)\n",
        "\n",
        "# Reading new data as dataframe\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Data Mining/CA 2/Unlabelled.csv\")\n",
        "pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells\n",
        "pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window\n",
        "\n",
        "# Cleaning reviews\n",
        "def cleaner(Text):\n",
        "    soup = BeautifulSoup(Text, 'lxml') # removing HTML entities such as â€˜&ampâ€™,â€™&quotâ€™,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n",
        "    souped = soup.get_text()\n",
        "    re1 = re.sub(r\"(@|http://|https://|www|\\\\x)\\S*\", \" \", souped) # substituting @mentions, urls, etc with whitespace\n",
        "    re2 = re.sub(\"[^A-Za-z]+\",\" \", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace\n",
        "\n",
        "    \"\"\"\n",
        "    For more info on regular expressions visit -\n",
        "    https://docs.python.org/3/howto/regex.html\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = nltk.word_tokenize(re2)\n",
        "    lower_case = [t.lower() for t in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "    return lemmas\n",
        "\n",
        "df['Cleaned_review'] = df.Text.apply(cleaner)\n",
        "df = df[df['Cleaned_review'].map(len) > 0] # removing rows with cleaned text of length 0\n",
        "print(\"Printing top 5 rows of dataframe showing original and cleaned Texts....\")\n",
        "print(df[['Text','Cleaned_review']].head())\n",
        "df['Cleaned_review'] = [\" \".join(row) for row in df['Cleaned_review'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n",
        "data = df['Cleaned_review']\n",
        "# tfidf.fit(data)\n",
        "# data_tfidf = tfidf.transform(data)\n",
        "# y_pred = model.predict(data_tfidf)\n",
        "\n",
        "y_pred = [model.predict(text)[0][0] for text in df['Cleaned_review']]\n",
        "y_pred = [label.split('__')[-1] for label in y_pred]\n",
        "\n",
        "df['Predicted_Review'] = y_pred\n",
        "df.to_csv('Predicted_Review.csv', index=False)\n",
        "\n",
        "# #### Saving predicted ratings to csv\n",
        "# df['Predicted_Review'] = y_pred.reshape(-1,1)\n",
        "# df.to_csv('Predicted_Review.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BkM_oB0a-J2",
        "outputId": "254e6ef8-90ec-4148-d420-f87d4b5524b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'aroma': 0, 'aroma eco': 1, 'aroma eco friendly': 2, 'aroma good': 3, 'aroma good value': 4, 'aroma highly': 5, 'aroma highly recommend': 6, 'aroma improve': 7, 'aroma improve oral': 8, 'aroma long': 9, 'aroma long last': 10, 'aroma perfect': 11, 'aroma perfect daily': 12, 'aroma pleasant': 13, 'aroma pleasant taste': 14, 'aroma refresh': 15, 'aroma whiten': 16, 'bad': 17, 'bad smell': 18, 'bad smell cause': 19, 'bad smell daily': 20, 'bad smell harmful': 21, 'bad smell ineffective': 22, 'bad smell overprice': 23, 'bad smell recommend': 24, 'bad smell short': 25, 'bad smell strong': 26, 'bad smell whiten': 27, 'besttoothpaste': 28, 'breath': 29, 'breath eco': 30, 'breath eco friendly': 31, 'breath gentle': 32, 'breath gentle gum': 33, 'breath good': 34, 'breath good value': 35, 'breath highly': 36, 'breath highly recommend': 37, 'breath improve': 38, 'breath long': 39, 'breath long last': 40, 'breath perfect': 41, 'breath pleasant': 42, 'breath pleasant aroma': 43, 'breath pleasant taste': 44, 'breath quality': 45, 'cause': 46, 'cause irritation': 47, 'cause irritation daily': 48, 'cause irritation harmful': 49, 'cause irritation harsh': 50, 'cause irritation ineffective': 51, 'cause irritation overprice': 52, 'cause irritation short': 53, 'cause irritation strong': 54, 'couldbebetter': 55, 'daily': 56, 'daily use': 57, 'daily use bad': 58, 'daily use cause': 59, 'daily use gentle': 60, 'daily use good': 61, 'daily use harmful': 62, 'daily use harsh': 63, 'daily use highly': 64, 'daily use improve': 65, 'daily use ineffective': 66, 'daily use pleasant': 67, 'daily use poor': 68, 'daily use refresh': 69, 'daily use short': 70, 'daily use whiten': 71, 'dentalhealth': 72, 'dentition': 73, 'disappoint': 74, 'eco': 75, 'eco friendly': 76, 'eco friendly package': 77, 'ecofriendly': 78, 'effect': 79, 'effect bad': 80, 'effect bad smell': 81, 'effect eco': 82, 'effect eco friendly': 83, 'effect good': 84, 'effect good value': 85, 'effect harsh': 86, 'effect harsh gum': 87, 'effect improve': 88, 'effect improve oral': 89, 'effect ineffective': 90, 'effect overprice': 91, 'effect perfect': 92, 'effect perfect daily': 93, 'effect pleasant': 94, 'effect pleasant taste': 95, 'effect poor': 96, 'effect poor package': 97, 'effect recommend': 98, 'effect refresh': 99, 'effect strong': 100, 'effect strong taste': 101, 'effect whiten': 102, 'effect whiten teeth': 103, 'entry': 104, 'entry review': 105, 'entry review text': 106, 'fluoridefree': 107, 'freshbreath': 108, 'friendly': 109, 'friendly package': 110, 'friendly package eco': 111, 'friendly package good': 112, 'friendly package highly': 113, 'friendly package improve': 114, 'friendly package long': 115, 'friendly package perfect': 116, 'friendly package pleasant': 117, 'friendly package quality': 118, 'friendly package refresh': 119, 'friendly package whiten': 120, 'gentle': 121, 'gentle gingiva': 122, 'gentle gum': 123, 'gentle gum perfect': 124, 'gentle gum pleasant': 125, 'gentle gum quality': 126, 'gentle gum whiten': 127, 'gingiva': 128, 'good': 129, 'good value': 130, 'good value eco': 131, 'good value gentle': 132, 'good value highly': 133, 'good value improve': 134, 'good value perfect': 135, 'good value pleasant': 136, 'good value quality': 137, 'good value whiten': 138, 'gum': 139, 'gum bad': 140, 'gum bad smell': 141, 'gum cause': 142, 'gum cause irritation': 143, 'gum ineffective': 144, 'gum overprice': 145, 'gum perfect': 146, 'gum perfect daily': 147, 'gum pleasant': 148, 'gum pleasant aroma': 149, 'gum poor': 150, 'gum quality': 151, 'gum quality product': 152, 'gum recommend': 153, 'gum short': 154, 'gum short effect': 155, 'gum whiten': 156, 'gum whiten teeth': 157, 'harmful': 158, 'harmful oral': 159, 'harmful oral health': 160, 'harsh': 161, 'harsh gingiva': 162, 'harsh gum': 163, 'harsh gum bad': 164, 'harsh gum cause': 165, 'harsh gum ineffective': 166, 'harsh gum overprice': 167, 'harsh gum poor': 168, 'harsh gum recommend': 169, 'harsh gum short': 170, 'health': 171, 'health bad': 172, 'health bad smell': 173, 'health cause': 174, 'health cause irritation': 175, 'health eco': 176, 'health eco friendly': 177, 'health good': 178, 'health good value': 179, 'health harsh': 180, 'health highly': 181, 'health highly recommend': 182, 'health ineffective': 183, 'health long': 184, 'health long last': 185, 'health overprice': 186, 'health perfect': 187, 'health pleasant': 188, 'health pleasant aroma': 189, 'health pleasant taste': 190, 'health quality': 191, 'health quality product': 192, 'health recommend': 193, 'health refresh': 194, 'health refresh breath': 195, 'health short': 196, 'health short effect': 197, 'health strong': 198, 'health strong taste': 199, 'health whiten': 200, 'health whiten teeth': 201, 'highly': 202, 'highly recommend': 203, 'highly recommend eco': 204, 'highly recommend gentle': 205, 'highly recommend good': 206, 'highly recommend improve': 207, 'highly recommend perfect': 208, 'highly recommend pleasant': 209, 'highly recommend quality': 210, 'highly recommend refresh': 211, 'highly recommend whiten': 212, 'improve': 213, 'improve oral': 214, 'improve oral health': 215, 'ineffective': 216, 'ineffective cause': 217, 'ineffective cause irritation': 218, 'ineffective daily': 219, 'ineffective daily use': 220, 'ineffective harmful': 221, 'ineffective harmful oral': 222, 'ineffective harsh': 223, 'ineffective poor': 224, 'ineffective poor package': 225, 'ineffective recommend': 226, 'ineffective short': 227, 'ineffective short effect': 228, 'ineffective strong': 229, 'ineffective strong taste': 230, 'ineffective whiten': 231, 'irritation': 232, 'irritation daily': 233, 'irritation daily use': 234, 'irritation harmful': 235, 'irritation harmful oral': 236, 'irritation harsh': 237, 'irritation harsh gum': 238, 'irritation ineffective': 239, 'irritation overprice': 240, 'irritation short': 241, 'irritation short effect': 242, 'irritation strong': 243, 'irritation strong taste': 244, 'last': 245, 'last effect': 246, 'last effect eco': 247, 'last effect good': 248, 'last effect improve': 249, 'last effect perfect': 250, 'last effect pleasant': 251, 'last effect refresh': 252, 'last effect whiten': 253, 'long': 254, 'long last': 255, 'long last effect': 256, 'minty': 257, 'miss': 258, 'miss entry': 259, 'miss entry review': 260, 'naturalingredients': 261, 'notrecommended': 262, 'oral': 263, 'oral health': 264, 'oral health bad': 265, 'oral health cause': 266, 'oral health eco': 267, 'oral health good': 268, 'oral health harsh': 269, 'oral health highly': 270, 'oral health ineffective': 271, 'oral health long': 272, 'oral health overprice': 273, 'oral health perfect': 274, 'oral health pleasant': 275, 'oral health quality': 276, 'oral health recommend': 277, 'oral health refresh': 278, 'oral health short': 279, 'oral health strong': 280, 'oral health whiten': 281, 'oralcare': 282, 'overprice': 283, 'overprice bad': 284, 'overprice bad smell': 285, 'overprice daily': 286, 'overprice daily use': 287, 'overprice harmful': 288, 'overprice harmful oral': 289, 'overprice harsh': 290, 'overprice ineffective': 291, 'overprice poor': 292, 'overprice poor package': 293, 'overprice recommend': 294, 'overprice short': 295, 'overprice short effect': 296, 'overprice strong': 297, 'overprice strong taste': 298, 'overprice whiten': 299, 'package': 300, 'package bad': 301, 'package bad smell': 302, 'package cause': 303, 'package cause irritation': 304, 'package daily': 305, 'package daily use': 306, 'package eco': 307, 'package eco friendly': 308, 'package good': 309, 'package good value': 310, 'package harmful': 311, 'package harmful oral': 312, 'package harsh': 313, 'package harsh gum': 314, 'package highly': 315, 'package improve': 316, 'package improve oral': 317, 'package ineffective': 318, 'package long': 319, 'package long last': 320, 'package perfect': 321, 'package perfect daily': 322, 'package pleasant': 323, 'package pleasant aroma': 324, 'package pleasant taste': 325, 'package quality': 326, 'package quality product': 327, 'package recommend': 328, 'package refresh': 329, 'package refresh breath': 330, 'package short': 331, 'package short effect': 332, 'package strong': 333, 'package strong taste': 334, 'package whiten': 335, 'perfect': 336, 'perfect daily': 337, 'perfect daily use': 338, 'pleasant': 339, 'pleasant aroma': 340, 'pleasant aroma eco': 341, 'pleasant aroma good': 342, 'pleasant aroma highly': 343, 'pleasant aroma improve': 344, 'pleasant aroma long': 345, 'pleasant aroma perfect': 346, 'pleasant aroma pleasant': 347, 'pleasant aroma refresh': 348, 'pleasant aroma whiten': 349, 'pleasant taste': 350, 'pleasant taste eco': 351, 'pleasant taste gentle': 352, 'pleasant taste good': 353, 'pleasant taste highly': 354, 'pleasant taste perfect': 355, 'pleasant taste pleasant': 356, 'pleasant taste quality': 357, 'pleasant taste refresh': 358, 'pleasant taste whiten': 359, 'poor': 360, 'poor package': 361, 'poor package bad': 362, 'poor package cause': 363, 'poor package daily': 364, 'poor package harmful': 365, 'poor package harsh': 366, 'poor package ineffective': 367, 'poor package recommend': 368, 'poor package short': 369, 'poor package strong': 370, 'poor package whiten': 371, 'product': 372, 'product eco': 373, 'product eco friendly': 374, 'product gentle': 375, 'product good': 376, 'product good value': 377, 'product highly': 378, 'product highly recommend': 379, 'product improve': 380, 'product improve oral': 381, 'product perfect': 382, 'product perfect daily': 383, 'product pleasant': 384, 'product pleasant aroma': 385, 'product pleasant taste': 386, 'product refresh': 387, 'product refresh breath': 388, 'quality': 389, 'quality product': 390, 'quality product eco': 391, 'quality product gentle': 392, 'quality product good': 393, 'quality product highly': 394, 'quality product improve': 395, 'quality product perfect': 396, 'quality product pleasant': 397, 'quality product refresh': 398, 'recommend': 399, 'recommend bad': 400, 'recommend bad smell': 401, 'recommend cause': 402, 'recommend cause irritation': 403, 'recommend daily': 404, 'recommend daily use': 405, 'recommend eco': 406, 'recommend eco friendly': 407, 'recommend gentle': 408, 'recommend good': 409, 'recommend good value': 410, 'recommend harmful': 411, 'recommend harmful oral': 412, 'recommend harsh': 413, 'recommend harsh gum': 414, 'recommend improve': 415, 'recommend improve oral': 416, 'recommend ineffective': 417, 'recommend overprice': 418, 'recommend perfect': 419, 'recommend perfect daily': 420, 'recommend pleasant': 421, 'recommend pleasant aroma': 422, 'recommend pleasant taste': 423, 'recommend poor': 424, 'recommend poor package': 425, 'recommend quality': 426, 'recommend quality product': 427, 'recommend refresh': 428, 'recommend refresh breath': 429, 'recommend short': 430, 'recommend short effect': 431, 'recommend whiten': 432, 'refresh': 433, 'refresh breath': 434, 'refresh breath eco': 435, 'refresh breath gentle': 436, 'refresh breath good': 437, 'refresh breath highly': 438, 'refresh breath improve': 439, 'refresh breath long': 440, 'refresh breath perfect': 441, 'refresh breath pleasant': 442, 'refresh breath quality': 443, 'review': 444, 'review text': 445, 'review text miss': 446, 'short': 447, 'short effect': 448, 'short effect bad': 449, 'short effect harsh': 450, 'short effect ineffective': 451, 'short effect overprice': 452, 'short effect poor': 453, 'short effect recommend': 454, 'short effect strong': 455, 'short effect whiten': 456, 'smell': 457, 'smell cause': 458, 'smell cause irritation': 459, 'smell daily': 460, 'smell daily use': 461, 'smell harmful': 462, 'smell harmful oral': 463, 'smell ineffective': 464, 'smell overprice': 465, 'smell recommend': 466, 'smell short': 467, 'smell short effect': 468, 'smell strong': 469, 'smell strong taste': 470, 'smell whiten': 471, 'smile': 472, 'strong': 473, 'strong taste': 474, 'strong taste bad': 475, 'strong taste cause': 476, 'strong taste daily': 477, 'strong taste harmful': 478, 'strong taste overprice': 479, 'strong taste poor': 480, 'strong taste recommend': 481, 'strong taste whiten': 482, 'sustainable': 483, 'taste': 484, 'taste bad': 485, 'taste bad smell': 486, 'taste cause': 487, 'taste cause irritation': 488, 'taste daily': 489, 'taste daily use': 490, 'taste eco': 491, 'taste eco friendly': 492, 'taste gentle': 493, 'taste good': 494, 'taste good value': 495, 'taste harmful': 496, 'taste harmful oral': 497, 'taste highly': 498, 'taste highly recommend': 499, 'taste overprice': 500, 'taste perfect': 501, 'taste perfect daily': 502, 'taste pleasant': 503, 'taste pleasant aroma': 504, 'taste poor': 505, 'taste poor package': 506, 'taste quality': 507, 'taste quality product': 508, 'taste recommend': 509, 'taste refresh': 510, 'taste whiten': 511, 'teeth': 512, 'teeth eco': 513, 'teeth eco friendly': 514, 'teeth good': 515, 'teeth good value': 516, 'teeth highly': 517, 'teeth highly recommend': 518, 'teeth improve': 519, 'teeth improve oral': 520, 'teeth perfect': 521, 'teeth perfect daily': 522, 'teeth pleasant': 523, 'teeth pleasant aroma': 524, 'teeth refresh': 525, 'teeth refresh breath': 526, 'teethwhitening': 527, 'text': 528, 'text miss': 529, 'text miss entry': 530, 'use': 531, 'use bad': 532, 'use bad smell': 533, 'use cause': 534, 'use gentle': 535, 'use good': 536, 'use good value': 537, 'use harmful': 538, 'use harmful oral': 539, 'use harsh': 540, 'use highly': 541, 'use highly recommend': 542, 'use improve': 543, 'use improve oral': 544, 'use ineffective': 545, 'use pleasant': 546, 'use poor': 547, 'use poor package': 548, 'use refresh': 549, 'use refresh breath': 550, 'use short': 551, 'use short effect': 552, 'use whiten': 553, 'value': 554, 'value eco': 555, 'value eco friendly': 556, 'value gentle': 557, 'value highly': 558, 'value highly recommend': 559, 'value improve': 560, 'value improve oral': 561, 'value perfect': 562, 'value perfect daily': 563, 'value pleasant': 564, 'value pleasant aroma': 565, 'value pleasant taste': 566, 'value quality': 567, 'value quality product': 568, 'value whiten': 569, 'whiten': 570, 'whiten bad': 571, 'whiten bad smell': 572, 'whiten cause': 573, 'whiten cause irritation': 574, 'whiten dentition': 575, 'whiten harmful': 576, 'whiten harmful oral': 577, 'whiten ineffective': 578, 'whiten overprice': 579, 'whiten poor': 580, 'whiten poor package': 581, 'whiten recommend': 582, 'whiten strong': 583, 'whiten strong taste': 584, 'whiten teeth': 585, 'whiten teeth eco': 586, 'whiten teeth good': 587, 'whiten teeth highly': 588, 'whiten teeth improve': 589, 'whiten teeth perfect': 590, 'whiten teeth pleasant': 591, 'whiten teeth refresh': 592}\n",
            "Printing top 5 rows of dataframe showing original and cleaned Texts....\n",
            "                                                                                                                                                                                                                Text  \\\n",
            "0  In [language], we say: This toothpaste cau(sed irritation in my mouth, not suit<able for sensitive pearly whites. pearly cau(sed toothpaste not cau(sed toothpaste pearly pearly toothpaste for pearly toothpaste   \n",
            "1                                                                          This is the best toothpast*e I've used! It keeps my teeth healthy and shining. &teeth healthy the I've used! my healthy keeps teeth my my   \n",
            "2                                              The texture >is unpleasant and I didn't notice any whitening effect. whitening notice whitening The I whitening unpl;easant The didn't I whitening whitening I notice   \n",
            "3                                                                       This is the) best toothpaste I've used! It keeps my pearly whites healthy and shining:. the It keeps It best It my pearly whites It keeps my   \n",
            "4                                                                              In [language], we say: I won't be buying }this dental cream? again - it left a weird aftertaste. - weird - - left - weird - - won't a   \n",
            "\n",
            "                                                                                                                          Cleaned_review  \n",
            "0                                                                         [language, say, toothpaste, cau, sed, irritation, mouth, suit]  \n",
            "1                                  [best, toothpast, e, used, keep, teeth, healthy, shining, teeth, healthy, used, healthy, keep, teeth]  \n",
            "2  [texture, unpleasant, notice, whitening, effect, whitening, notice, whitening, whitening, unpl, easant, whitening, whitening, notice]  \n",
            "3                                       [best, toothpaste, used, keep, pearly, white, healthy, shining, keep, best, pearly, white, keep]  \n",
            "4                                                    [language, say, buying, dental, cream, left, weird, aftertaste, weird, left, weird]  \n"
          ]
        }
      ]
    }
  ]
}